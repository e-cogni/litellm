model_list:
# OpenAI
  - model_name: openai-dall-e-3
    litellm_params:
      model: openai/dall-e-3
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: image_generation
      base_model: dall-e-3
  - model_name: openai-dall-e-2
    litellm_params:
      model: openai/dall-e-2
      api_key: os.environ/OPENAI_API_KEY
    model_info:
      mode: image_generation
      base_model: dall-e-2
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: openai/gpt-3.5-turbo   
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-3.5-turbo-instruct
    litellm_params:
      model: text-completion-openai/gpt-3.5-turbo-instruct
      api_key: os.environ/OPENAI_API_KEY
  - model_name: gpt-3.5-turbo-large
    litellm_params: 
      model: "gpt-3.5-turbo-1106"
      api_key: os.environ/OPENAI_API_KEY
      rpm: 480
      timeout: 300
      stream_timeout: 60
  - model_name: "davinci"
    litellm_params:
      model: "openai/davinci-002"
      api_key: os.environ/OPENAI_API_KEY

# Grog
  - model_name: "groq/*"
    litellm_params:
      model: "groq/*"
      api_key: os.environ/GROQ_API_KEY

# Cloudflare
  - model_name: "phi-2"
    litellm_params:
      model: "openai/@cf/microsoft/phi-2"
      api_key: os.environ/CLOUDFLARE_API_KEY
      api_base: os.environ/CLOUDFLARE_API_BASE
  - model_name: "starling-lm-7b"
    litellm_params:
      model: "openai/@hf/nexusflow/starling-lm-7b-beta"
      api_key: os.environ/CLOUDFLARE_API_KEY
      api_base: os.environ/CLOUDFLARE_API_BASE
    model_info:
      supports_vision: True
  - model_name: "mistral-7b"
    litellm_params:
      model: "openai/@hf/mistral/mistral-7b-instruct-v0.2"
      api_key: os.environ/CLOUDFLARE_API_KEY
      api_base: os.environ/CLOUDFLARE_API_BASE
  - model_name: "llama-3.2-11b"
    litellm_params:
      model: "openai/@cf/meta/llama-3.2-11b-vision-instruct"
      api_key: os.environ/CLOUDFLARE_API_KEY
      api_base: os.environ/CLOUDFLARE_API_BASE
    model_info:
      supports_vision: True
  - model_name: "llama-3.2-3b"
    litellm_params:
      model: "openai/@cf/meta/llama-3.2-3b-instruct"
      api_key: os.environ/CLOUDFLARE_API_KEY
      api_base: os.environ/CLOUDFLARE_API_BASE
      max_tokens: 4096
  - model_name: "llama-2-7b-chat-fp16"
    litellm_params:
      model: "openai/@cf/meta/llama-2-7b-chat-fp16"
      api_key: os.environ/CLOUDFLARE_API_KEY
      api_base: os.environ/CLOUDFLARE_API_BASE    

#Github
  - model_name: "mistral-large"
    litellm_params:
      model: "github/Mistral-large-2407"
      api_key: os.environ/GITHUB_API_KEY
  - model_name: "cohere-plus"
    litellm_params:
      model: "github/Cohere-command-r-plus-08-2024"
      api_key: os.environ/GITHUB_API_KEY
  - model_name: "phi-3"
    litellm_params:
      model: "github/Phi-3-medium-128k-instruct"
      api_key: os.environ/GITHUB_API_KEY
  - model_name: "phi-3.5-MoE"
    litellm_params:
      model: "github/Phi-3.5-MoE-instruct"
      api_key: os.environ/GITHUB_API_KEY
  - model_name: "gpt-4o"
    litellm_params:
      model: "github/gpt-4o"
      api_key: os.environ/GITHUB_API_KEY
    model_info:
      supports_vision: True
  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "github/gpt-4o-mini"
      api_key: os.environ/GITHUB_API_KEY
    model_info:
      supports_vision: True
  - model_name: "jamba"
    litellm_params:
      model: "github/AI21-Jamba-1.5-Large"
      api_key: os.environ/GITHUB_API_KEY

litellm_settings:
  # set_verbose: True  # Uncomment this if you want to see verbose logs; not recommended in production
  drop_params: True
  # max_budget: 100 
  # budget_duration: 30d
  num_retries: 5
  request_timeout: 600
  telemetry: True
  cache: True
  callbacks: ["otel", "langfuse"]
  input_callback: ["sentry", "langfuse"]
  failure_callback: ["sentry", "langfuse"]
#  context_window_fallbacks: [{"llama3-8b": ["llama3-8b-8192"]}]
 # default_team_settings: 
 #   - team_id: team-1
 #     success_callback: ["langfuse"]
 #     failure_callback: ["langfuse"]
 #     langfuse_public_key: os.environ/LANGFUSE_PROJECT1_PUBLIC # Project 1
 #     langfuse_secret: os.environ/LANGFUSE_PROJECT1_SECRET # Project 1
 #   - team_id: team-2
 #     success_callback: ["langfuse"]
 #     failure_callback: ["langfuse"]
 #     langfuse_public_key: os.environ/LANGFUSE_PROJECT2_PUBLIC # Project 2
 #     langfuse_secret: os.environ/LANGFUSE_PROJECT2_SECRET # Project 2
 #     langfuse_host: https://us.cloud.langfuse.com

# For /fine_tuning/jobs endpoints
#finetune_settings:
#  - custom_llm_provider: azure
#    api_base: https://exampleopenaiendpoint-production.up.railway.app
#    api_key: fake-key
#    api_version: "2023-03-15-preview"
#  - custom_llm_provider: openai
#    api_key: os.environ/OPENAI_API_KEY

# for /files endpoints
#files_settings:
#  - custom_llm_provider: azure
#    api_base: https://exampleopenaiendpoint-production.up.railway.app
#    api_key: fake-key
#    api_version: "2023-03-15-preview"
#  - custom_llm_provider: openai
#    api_key: os.environ/OPENAI_API_KEY

router_settings:
  routing_strategy: usage-based-routing-v2 
  redis_host: os.environ/REDIS_HOST
  redis_password: os.environ/REDIS_PASSWORD
  redis_port: os.environ/REDIS_PORT
#  enable_pre_call_checks: true
#  model_group_alias: {"my-special-fake-model-alias-name": "fake-openai-endpoint-3"} 

general_settings: 
  master_key: os.environ/LITELLM_MASTER_KEY # [OPTIONAL] Use to enforce auth on proxy. See - https://docs.litellm.ai/docs/proxy/virtual_keys
  store_model_in_db: True
  proxy_budget_rescheduler_min_time: 60
  proxy_budget_rescheduler_max_time: 64
  proxy_batch_write_at: 1
  database_connection_pool_limit: 10
  database_url: os.environ/DATABASE_URL # [OPTIONAL] use for token-based auth to proxy

  pass_through_endpoints:
    - path: "/v1/rerank"                                  # route you want to add to LiteLLM Proxy Server
      target: "https://api.cohere.com/v1/rerank"          # URL this route should forward requests to
      headers:                                            # headers to forward to this URL
        content-type: application/json                    # (Optional) Extra Headers to pass to this endpoint 
        accept: application/json
      forward_headers: True

# environment_variables:
  # settings for using redis caching
  # REDIS_HOST: redis-16337.c322.us-east-1-2.ec2.cloud.redislabs.com
  # REDIS_PORT: "16337"
  # REDIS_PASSWORD: 
